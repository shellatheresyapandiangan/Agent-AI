{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO0mqaUsItBv"
   },
   "source": [
    "# Contextual Semantic Search with Chroma and LiteLLM\n",
    "\n",
    "This notebook demonstrates how to build a **Contextual Semantic Search** system using **Chroma Vector Database** and **LiteLLM** with the **Gemini model**. The system processes a folder of PDF documents, builds a knowledge base, and retrieves contextually relevant information to answer user queries. Below is an overview of the tools and methods used.\n",
    "\n",
    "---\n",
    "\n",
    "## Tools Used\n",
    "\n",
    "1. **Chroma Vector Database**:\n",
    "   - A vector database designed for storing and querying embeddings.\n",
    "   - Enables efficient similarity search for retrieving semantically relevant documents.\n",
    "\n",
    "2. **Sentence Transformers**:\n",
    "   - A library for generating high-quality embeddings (vector representations) of text.\n",
    "   - Uses pre-trained models like `all-MiniLM-L6-v2` to convert text into embeddings.\n",
    "\n",
    "3. **LangChain**:\n",
    "   - A framework for working with text data, including text splitting and chunking.\n",
    "   - The `RecursiveCharacterTextSplitter` is used to split text into semantically meaningful chunks.\n",
    "\n",
    "4. **LiteLLM**:\n",
    "   - A lightweight library for interacting with large language models (LLMs).\n",
    "   - Used here to call the **Gemini model** for generating responses based on retrieved context.\n",
    "\n",
    "5. **PyPDF2**:\n",
    "   - A library for extracting text from PDF files.\n",
    "   - Used to process all PDFs in a specified folder.\n",
    "\n",
    "---\n",
    "\n",
    "## Methods and Steps\n",
    "\n",
    "### Step 1: Extract Text from PDFs\n",
    "- All PDF files in a specified folder are processed using **PyPDF2**.\n",
    "- The text from each PDF is extracted and combined into a single string.\n",
    "\n",
    "### Step 2: Split Text into Chunks\n",
    "- The combined text is split into smaller chunks using **LangChain's RecursiveCharacterTextSplitter**.\n",
    "- This ensures that the chunks are semantically meaningful and retain context.\n",
    "\n",
    "### Step 3: Generate Embeddings\n",
    "- Each text chunk is converted into an embedding (vector representation) using **Sentence Transformers**.\n",
    "- The embeddings capture the semantic meaning of the text.\n",
    "\n",
    "### Step 4: Build the Knowledge Base\n",
    "- The embeddings and corresponding text chunks are stored in **Chroma Vector Database**.\n",
    "- This allows for efficient similarity searches based on semantic meaning.\n",
    "\n",
    "### Step 5: Perform Semantic Search\n",
    "- A user query is converted into an embedding.\n",
    "- The most semantically similar chunks are retrieved from the Chroma database.\n",
    "\n",
    "### Step 6: Generate Responses\n",
    "- The retrieved chunks are passed as context to the **Gemini model** via **LiteLLM**.\n",
    "- The model generates a response based on the query and the provided context.\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Input**: A folder containing PDF documents.\n",
    "2. **Processing**:\n",
    "   - Extract text from PDFs.\n",
    "   - Split text into chunks.\n",
    "   - Generate embeddings and store them in Chroma.\n",
    "3. **Query Handling**:\n",
    "   - Convert the query into an embedding.\n",
    "   - Retrieve the most relevant chunks from Chroma.\n",
    "   - Generate a response using the Gemini model.\n",
    "4. **Output**: A contextually accurate answer to the user's query.\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Approach?\n",
    "\n",
    "- **Contextual Semantic Search**: Goes beyond keyword matching to understand the meaning and context of the query.\n",
    "- **Efficient Retrieval**: Chroma enables fast and scalable similarity searches.\n",
    "- **High-Quality Responses**: The Gemini model generates accurate and contextually relevant answers.\n",
    "\n",
    "---\n",
    "\n",
    "Let’s get started! You can see the full workflow in the diagram below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5L1vQY0A8Cx"
   },
   "source": [
    "
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDXa6I0vJko0"
   },
   "source": [
    "## Library Installation\n",
    "\n",
    "Start by installing the library used in this Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3WyvhIh4CF_S"
   },
   "outputs": [],
   "source": [
    "!pip install -q chromadb pypdf2 sentence-transformers litellm langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blp6vej-LhJ0"
   },
   "source": [
    "## Import the Libraries and set the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0wMfOOAcGDy-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import litellm\n",
    "from litellm import completion\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# # Set environment variables. Uncomment this if you want to set them directly.\n",
    "# os.environ[\"HUGGINGFACE_TOKEN\"] = \"your_huggingface_token_here\"\n",
    "# os.environ[\"GEMINI_API_KEY\"] = \"your_gemini_api_key_here\"\n",
    "os.environ['LITELLM_LOG'] = 'DEBUG'\n",
    "\n",
    "# # Retrieve environment variables\n",
    "# HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "# GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbUoA9A_L7W0"
   },
   "source": [
    "## Extract Text from folder containing PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1p3vA3fwLmdU"
   },
   "outputs": [],
   "source": [
    "def extract_text_from_pdfs(folder_path):\n",
    "    all_text = \"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                for page in reader.pages:\n",
    "                    all_text += page.extract_text()\n",
    "    return all_text\n",
    "\n",
    "pdf_folder = \"dataset\"\n",
    "all_text = extract_text_from_pdfs(pdf_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTMQPIYbNSoC"
   },
   "source": [
    "## Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4jNqPkRMoWa"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Size of each chunk\n",
    "    chunk_overlap=50,  # Overlap between chunks to maintain context\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Splitting hierarchy\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQSCLY4aPweA"
   },
   "source": [
    "## Set up the Knowledge Base with ChromaDB and Generate Embeddings with sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hPA5OziJNQ5L",
    "outputId": "d1ea896e-a2a8-4bf3-91a6-3328428cca1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing collection: knowledge_base\n"
     ]
    }
   ],
   "source": [
    "# Initialize a persistent ChromaDB client\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "\n",
    "# Load the SentenceTransformer model for text embeddings\n",
    "text_embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Delete existing collection (if needed)\n",
    "try:\n",
    "    client.delete_collection(name=\"knowledge_base\")\n",
    "    print(\"Deleted existing collection: knowledge_base\")\n",
    "except Exception as e:\n",
    "    print(f\"Collection does not exist or could not be deleted: {e}\")\n",
    "\n",
    "# Create a new collection for text embeddings\n",
    "collection = client.create_collection(name=\"knowledge_base\")\n",
    "\n",
    "# Add text chunks to the collection\n",
    "for i, chunk in enumerate(chunks):\n",
    "    # Generate embeddings for the chunk\n",
    "    embedding = text_embedding_model.encode(chunk)\n",
    "\n",
    "    # Add to the collection with metadata\n",
    "    collection.add(\n",
    "        ids=[f\"chunk_{i}\"],  # Unique ID for each chunk\n",
    "        embeddings=[embedding.tolist()],  # Embedding vector\n",
    "        metadatas=[{\"source\": \"pdf\", \"chunk_id\": i}],  # Metadata\n",
    "        documents=[chunk]  # Original text\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SseM1yu8S08_"
   },
   "source": [
    "## Perform Semantic Search with ChromaDB and Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "518TSfLISRZ_",
    "outputId": "4e4cedc2-d2b6-4217-ed96-017bc358132d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: insurance) \n",
      "FINANCIAL RESPONSIBILITY LAW\n",
      "A state law requiring that all automobile \n",
      "drivers show proof that they can pay dam-\n",
      "ages up to a minimum amount if involved \n",
      "in an auto accident. Varies from state to \n",
      "state but can be met by carrying a mini-\n",
      "mum amount of auto liability insurance. \n",
      "(See Compulsory auto insurance)\n",
      "FINITE RISK REINSURANCE\n",
      "Contract under which the ultimate li-\n",
      "ability of the reinsurer is capped and on \n",
      "which anticipated investment income is\n",
      "\n",
      "Result 2: policyholder’s car from a collision. \n",
      "5. Comprehensive, for damage to the poli-\n",
      "cyholder’s car not involving a collision \n",
      "with another car (including damage \n",
      "from fire, explosions, earthquakes, floods, and riots), and theft. 6. Uninsured motorists coverage, for costs \n",
      "resulting from an accident involving a hit-and-run driver or a driver who does not have insurance. \n",
      "AUTO INSURANCE PREMIUM\n",
      "The price an insurance company charges \n",
      "for coverage, based on the frequency and\n",
      "\n",
      "Result 3: Overview\t I.I.I.\tInsurance\tHandbook\t\t\twww.iii.org/insurancehandbook\t \t3Auto Insurance Basics\n",
      "Auto insurance protects against financial loss in the event of an accident. It is a \n",
      "contract between the policyholder and the insurance company. The policyhold-er agrees to pay the premium and the insurance company agrees to pay losses as defined in the policy.\n",
      "Auto insurance provides property, liability and medical coverage:\n",
      "\tProperty coverage pays for damage to, or theft of, the car.\n",
      "\n",
      "Result 4: Insurers that are created and wholly owned \n",
      "by one or more non-insurers, to provide \n",
      "owners with coverage. A form of self-insur-\n",
      "ance.\n",
      "CAR YEAR\n",
      "Equal to 365 days of insured coverage for a \n",
      "single vehicle. It is the standard measure-\n",
      "ment for automobile insurance.\n",
      "CASE MANAGEMENT\n",
      "A system of coordinating medical services \n",
      "to treat a patient, improve care and reduce \n",
      "cost. A case manager coordinates health \n",
      "care delivery for patients.\n",
      "*CASH DIVIDEND OPTION\n",
      "\n",
      "Result 5: Most states require drivers to have auto liability insurance before they can legal-ly drive a car. (Liability insurance pays the other driver’s medical, car repair and other costs when the policyholder is at fault in an auto accident.) All states have laws that set the minimum amounts of insurance or other financial security drivers have to pay for the harm caused by their negligence behind the wheel if an accident occurs. Most auto policies are for six months to a year. A basic auto insurance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def semantic_search(query, top_k=5):\n",
    "    # Generate embedding for the query\n",
    "    query_embedding = text_embedding_model.encode(query)\n",
    "\n",
    "    # Query the collection\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding.tolist()],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# Example query\n",
    "query = \"What is the insurance for car?\"\n",
    "results = semantic_search(query)\n",
    "\n",
    "# Display results\n",
    "for i, result in enumerate(results['documents'][0]):\n",
    "    print(f\"Result {i+1}: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPZhODtZTMHq"
   },
   "source": [
    "## Generate Repsonse Based on Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WHggGtUpPzYS",
    "outputId": "f6c0a0f2-87ed-46e2-ed22-66645651ed44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:22:16 - LiteLLM:WARNING\u001b[0m: utils.py:325 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n",
      "WARNING:LiteLLM:`litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {}\n",
      "RAW RESPONSE:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"Based on the provided text, car insurance, or auto insurance, protects against financial loss from an accident.  It's a contract where the policyholder pays a premium, and the insurance company pays for losses as defined in the policy.  The insurance provides property, liability, and medical coverage.  Property coverage pays for damage to or theft of the car.  Liability insurance pays the other driver's costs if the policyholder is at fault.  The text also mentions comprehensive coverage (damage not involving a collision) and uninsured motorists coverage.\\n\"\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"model\"\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"citationMetadata\": {\n",
      "        \"citationSources\": [\n",
      "          {\n",
      "            \"startIndex\": 241,\n",
      "            \"endIndex\": 367,\n",
      "            \"uri\": \"http://www.dm-insurance.com/faqs.html\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"avgLogprobs\": -0.12344997852772206\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 559,\n",
      "    \"candidatesTokenCount\": 111,\n",
      "    \"totalTokenCount\": 670\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-1.5-flash\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Generated Response:\n",
      " Based on the provided text, car insurance, or auto insurance, protects against financial loss from an accident.  It's a contract where the policyholder pays a premium, and the insurance company pays for losses as defined in the policy.  The insurance provides property, liability, and medical coverage.  Property coverage pays for damage to or theft of the car.  Liability insurance pays the other driver's costs if the policyholder is at fault.  The text also mentions comprehensive coverage (damage not involving a collision) and uninsured motorists coverage.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up LiteLLM with Gemini\n",
    "\n",
    "def generate_response(query, context):\n",
    "    # Combine the query and context for the prompt\n",
    "    prompt = f\"Query: {query}\\nContext: {context}\\nAnswer:\"\n",
    "\n",
    "    # Call the Gemini model via LiteLLM\n",
    "    response = completion(\n",
    "        model=\"gemini/gemini-1.5-flash\",  # Use the Gemini model\n",
    "        messages=[{\"content\": prompt, \"role\": \"user\"}],\n",
    "        api_key= GEMINI_API_KEY\n",
    "    )\n",
    "\n",
    "    # Extract and return the generated text\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Retrieve the top results from semantic search\n",
    "search_results = semantic_search(query)\n",
    "context = \"\\n\".join(search_results['documents'][0])\n",
    "\n",
    "# Generate a response using the retrieved context\n",
    "response = generate_response(query, context)\n",
    "print(\"Generated Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sv_zMQZKTYDi"
   },
   "source": [
    "## Set up the Test Dataset for RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#find it in the generated dataset folder if you miss it\n",
    "df = pd.read_csv('evaluation_results_question_insurance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Prompt for Context Relevance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Define evaluation criteria\n",
    "chunk_validation_prompt_template = \"\"\"\n",
    "{task}\n",
    "{evaluation_criteria}\n",
    "\n",
    "Follow these steps to generate your evaluation:\n",
    "{evaluation_steps}\n",
    "\n",
    "Please respond using the following JSON schema:\n",
    "\n",
    "Answer = {json_format}\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Score' in your answer.\n",
    "\n",
    "{question}\n",
    "{context}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "rating_json_format = \"\"\"\n",
    "{\n",
    "    \"Evaluation\": \"your rationale for the rating, as a text\",\n",
    "    \"Score\": \"your rating, as a number between 1 and 5\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "question_template = \"\"\"Now here are the question (delimited by triple backticks)\n",
    "Question: ```{question}```\n",
    "\"\"\"\n",
    "\n",
    "context_template = \"\"\"here are the context (delimited by triple quotes).\n",
    "Context: \\\"\\\"\\\"{context}\\\"\\\"\\\"\\n\n",
    "\"\"\"\n",
    "\n",
    "# Tasks, evaluation criteria, and steps\n",
    "context_evaluation_task = \"\"\"You will be given a context and a question.\n",
    "Your task is to evaluate the question based on the given context and provide a score between 1 and 5 according to the following criteria:\"\"\"\n",
    "\n",
    "context_evaluation_task = \"\"\"- Score 1: The context does not provide sufficient information to answer the question in any way.\n",
    "- Score 2 or 3: The context provides some relevant information, but the question remains partially answerable, or is unclear/ambiguous.\n",
    "- Score 4: The context offers sufficient information to answer the question, but some minor details are missing or unclear.\n",
    "- Score 5: The context provides all necessary information to answer the question clearly and without ambiguity.\"\"\"\n",
    "\n",
    "context_evaluation_task = \"\"\"- Read the context and question carefully.\n",
    "- Analyse and evaluate the question based on the provided evaluation criteria.\n",
    "- Provide a scaled score between 1 and 5 that reflect your evaluation.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the LLM Evaluator for Evaluation based on the Generated Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "# Define the request and token limits\n",
    "REQUEST_LIMIT = 15  # Maximum requests per minute\n",
    "TOKEN_LIMIT = 1_000_000  # Maximum tokens per minute\n",
    "\n",
    "# Initialize counters for rate limiting\n",
    "start_time = time.time()\n",
    "requests_made = 0\n",
    "tokens_used = 0\n",
    "\n",
    "# Define the function to enforce rate limits\n",
    "def enforce_rate_limit(start_time, requests_made, tokens_used):\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if requests_made >= REQUEST_LIMIT or tokens_used >= TOKEN_LIMIT:\n",
    "        sleep_time = max(0, 60 - elapsed_time)  # Wait until 1 minute has passed\n",
    "        time.sleep(sleep_time)\n",
    "        return time.time(), 0, 0  # Reset counters\n",
    "    return start_time, requests_made, tokens_used\n",
    "\n",
    "# Define the function to evaluate groundness\n",
    "def evaluate_groundness(question, context):\n",
    "    global start_time, requests_made, tokens_used\n",
    "    \n",
    "    # Enforce rate limits\n",
    "    start_time, requests_made, tokens_used = enforce_rate_limit(start_time, requests_made, tokens_used)\n",
    "    \n",
    "    # Prepare the prompt for groundness evaluation\n",
    "    prompt = f\"\"\"\n",
    "    {groundedness_task}\n",
    "\n",
    "    {groundedness_eval}\n",
    "\n",
    "    {groundedness_steps}\n",
    "\n",
    "    Please respond using the following JSON schema:\n",
    "\n",
    "    {rating_json_format}\n",
    "\n",
    "    Now here is the question (delimited by triple backticks):\n",
    "    Question: ```{question}```\n",
    "\n",
    "    Here is the context (delimited by triple quotes):\n",
    "    Context: \\\"\\\"\\\"{context}\\\"\\\"\\\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the response using Gemini\n",
    "    response = generate_response(prompt, \"\")\n",
    "    \n",
    "    # Update the counters\n",
    "    requests_made += 1\n",
    "    tokens_used += len(prompt.split())  # Approximate token count\n",
    "    \n",
    "    # Parse the response\n",
    "    try:\n",
    "        # Extract JSON part from the response (if any)\n",
    "        json_start = response.find(\"{\")\n",
    "        json_end = response.rfind(\"}\") + 1\n",
    "        json_response = response[json_start:json_end]\n",
    "        evaluation = json.loads(json_response)\n",
    "        return evaluation\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"Evaluation\": \"Invalid JSON response\", \"Score\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping through questions and each chunk retrieved for judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    question = row['question']\n",
    "    \n",
    "    # Perform semantic search\n",
    "    search_results = semantic_search(question, top_k=2)\n",
    "    \n",
    "    # Evaluate each chunk\n",
    "    for i, context in enumerate(search_results['documents'][0]):\n",
    "        evaluation = evaluate_groundness(question, context)\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"context\": context,\n",
    "            \"evaluation\": evaluation\n",
    "        })\n",
    "    \n",
    "    # Enforce rate limits after each question\n",
    "    start_time, requests_made, tokens_used = enforce_rate_limit(start_time, requests_made, tokens_used)\n",
    "\n",
    "# Display the results\n",
    "for result in results:\n",
    "    print(f\"Question: {result['question']}\")\n",
    "    print(f\"Context: {result['context']}\")\n",
    "    print(f\"Evaluation: {result['evaluation']['Evaluation']}\")\n",
    "    print(f\"Score: {result['evaluation']['Score']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the DataFrame and save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results list to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Flatten the 'evaluation' column into separate columns\n",
    "results_df = pd.concat(\n",
    "    [results_df.drop(columns=[\"evaluation\"]), results_df[\"evaluation\"].apply(pd.Series)],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "results_df.rename(columns={\"Evaluation\": \"Evaluation_Rationale\", \"Score\": \"Evaluation_Score\"}, inplace=True)\n",
    "\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "results_df.to_csv(\"RAG_evaluation_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Set the tracking URI and experiment name\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "EXPERIMENT_NAME = \"RAG-Question-Evaluation-Insurance\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_name\", \"gemini-1.5-flash\")\n",
    "\n",
    "    groundedness_scores = rag_result['Evaluation_Score'].dropna()\n",
    "\n",
    "    mlflow.log_metric(\"average_groundedness_score\", groundedness_scores.mean())\n",
    "\n",
    "    print(\"Results logged to MLflow.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "RAG_Venv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
